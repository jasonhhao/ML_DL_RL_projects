{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e3cdaa0-67a5-4d25-a16e-6233c293fac1",
    "_uuid": "a1b284038a70348c7ccb1071fb5ce2926bc7bcd7"
   },
   "source": [
    "# SMS Spam Collection Dataset\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a89760f4-9075-42be-b4c4-c8c87d175c88",
    "_uuid": "4429a082ddabcb23261daec29ee1ae9e68ba2a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457,) (1115,) (4457,) (1115,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"../input/\"\n",
    "\n",
    "df = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\n",
    "\n",
    "# split into train and test\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    df.v2,\n",
    "    df.v1, \n",
    "    test_size=0.2, \n",
    "    random_state=0) \n",
    "\n",
    "print(data_train.shape, data_test.shape, labels_train.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "330bad81-be26-4c49-84f1-4dc244ea7830",
    "_uuid": "2d91a3632ac990b10dd7c1f8942c76ce9eb0ed10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['small,', 'dorm', 'morning:)', 'Summers', 'Dear', 'letters', 'wth', 'SEE', '430', 'afternon,']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def GetVocabulary(data): \n",
    "    vocab_set = set([])\n",
    "    for document in data:\n",
    "        words = document.split()\n",
    "        for word in words:\n",
    "            vocab_set.add(word) \n",
    "    return list(vocab_set)\n",
    "\n",
    "vocab_list = GetVocabulary(data_train)\n",
    "print (vocab_list[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "736f4959-3411-42ae-9c7a-89319e2d2089",
    "_uuid": "201b71cc79802d48f253cd3416e79c874ead9e47",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Document2Vector(vocab_list, data):\n",
    "    word_vector = np.zeros(len(vocab_list))\n",
    "    words = data.split()\n",
    "    for word in words:\n",
    "        if word in vocab_list:\n",
    "            word_vector[vocab_list.index(word)] += 1\n",
    "    return word_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "8a842f21-6332-4508-9d39-47db407d9b8f",
    "_uuid": "f8e259bb2ef260294478c4ec7929cdd10b68d84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13504\n"
     ]
    }
   ],
   "source": [
    "train_matrix = []\n",
    "for document in data_train.values:\n",
    "    word_vector = Document2Vector(vocab_list, document)\n",
    "    train_matrix.append(word_vector)\n",
    "\n",
    "print (len(train_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "fd3f30bf-4c01-464b-a619-ac789c86afd0",
    "_uuid": "8f5d6d6019a13bd41997f2ce24772bfed365f127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n",
      "3876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def NaiveBayes_train(train_matrix,labels_train):\n",
    "    num_docs = len(train_matrix) ##4457\n",
    "    num_words = len(train_matrix[0]) ##13504\n",
    "    \n",
    "    spam_vector_count = np.ones(num_words)\n",
    "    ham_vector_count = np.ones(num_words)  \n",
    "    spam_total_count = num_words ##13504\n",
    "    ham_total_count = num_words  ##13504                \n",
    "    \n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    for i in range(num_docs):\n",
    "        if labels_train[i] == 'spam':\n",
    "            ham_vector_count += train_matrix[i]\n",
    "            ham_total_count += sum(train_matrix[i])\n",
    "            ham_count += 1\n",
    "        else:\n",
    "            spam_vector_count += train_matrix[i]\n",
    "            spam_total_count += sum(train_matrix[i])\n",
    "            spam_count += 1\n",
    "    \n",
    "    print (ham_count)\n",
    "    print (spam_count)\n",
    "    \n",
    "    p_spam_vector = np.log(ham_vector_count/ham_total_count)\n",
    "    p_ham_vector = np.log(spam_vector_count/spam_total_count)\n",
    "    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs)\n",
    "    \n",
    "p_spam_vector, p_spam, p_ham_vector, p_ham = NaiveBayes_train(train_matrix, labels_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e9b66d8-3a28-403f-a076-a60563a3def4",
    "_uuid": "59ce8f01f805169f97a6de260492b64254a2dc2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e4415b37-0855-49bd-8926-afe63c668268",
    "_uuid": "1d737630d3f2df2b3d28355f57cabbf594c2bf23"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def Predict(test_word_vector,p_spam_vector, p_spam, p_ham_vector, p_ham):\n",
    "    \n",
    "    spam = sum(test_word_vector * p_spam_vector) + p_spam\n",
    "    ham = sum(test_word_vector * p_ham_vector) + p_ham\n",
    "    if spam > ham:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "predictions = []\n",
    "i = 0\n",
    "for document in data_test.values:\n",
    "    test_word_vector = Document2Vector(vocab_list, document)\n",
    "    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n",
    "    predictions.append(ans)\n",
    "\n",
    "print (len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a35df0c4-a0bf-40ef-a509-33647beb8f5c",
    "_uuid": "992364e313ed7a48f0e3bd124e8af5d7b9786965"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print (accuracy_score(labels_test, predictions))\n",
    "print (classification_report(labels_test, predictions))\n",
    "print (confusion_matrix(labels_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
